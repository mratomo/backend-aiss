#!/usr/bin/env python3
"""
Script para probar que Ollama est√° detectando y utilizando GPU correctamente.
Este script debe ejecutarse dentro del contenedor rag-agent o directamente desde el host
para verificar que el contenedor Ollama local est√° utilizando la GPU correctamente
para la validaci√≥n de consultas de bases de datos.
"""

import os
import json
import asyncio
import argparse
import aiohttp
import time
import sys

async def test_ollama_api(api_url):
    """Prueba la conexi√≥n al API de Ollama y lista los modelos disponibles"""
    print(f"Probando conexi√≥n a Ollama en {api_url}")
    
    try:
        async with aiohttp.ClientSession() as session:
            async with session.get(f"{api_url}/api/tags") as response:
                if response.status != 200:
                    print(f"‚ùå Error: No se pudo conectar a Ollama API (status: {response.status})")
                    return False, []
                
                data = await response.json()
                models = data.get('models', [])
                
                if models:
                    model_names = [m.get('name') for m in models]
                    print(f"‚úÖ Conexi√≥n correcta con el API de Ollama")
                    print(f"üìã Modelos disponibles: {', '.join(model_names)}")
                    return True, models
                else:
                    print(f"‚ö†Ô∏è No hay modelos disponibles en el servidor Ollama")
                    return True, []
    except Exception as e:
        print(f"‚ùå Error conectando con Ollama API: {e}")
        return False, []

async def test_gpu_inference(api_url, model, use_gpu=True):
    """Prueba la inferencia del modelo con o sin opciones de GPU"""
    print(f"\n{'üöÄ Probando inferencia con GPU' if use_gpu else 'üíª Probando inferencia con CPU'}")
    print(f"Modelo: {model}")
    
    # Prompt para prueba
    prompt = "Explica brevemente c√≥mo las GPU aceleran la inferencia de los modelos de lenguaje grandes (LLMs)"
    
    # Configuraci√≥n b√°sica
    options = {
        "temperature": 0.1,
        "num_predict": 300  # Limitar respuesta para prueba
    }
    
    # A√±adir opciones de GPU si corresponde
    if use_gpu:
        gpu_options = {
            "num_gpu": 1,       # Usar 1 GPU
            "f16_kv": True,     # Usar FP16 para KV-cache (menor uso de memoria)
            "mirostat": 2,      # Estabilizador de muestreo
        }
        options.update(gpu_options)
        print("Opciones GPU activadas:", json.dumps(gpu_options, indent=2))
    
    # Payload para la petici√≥n
    payload = {
        "model": model,
        "prompt": prompt,
        "system": "Eres un experto en hardware para inteligencia artificial. Da respuestas concisas y t√©cnicamente precisas.",
        "options": options,
        "stream": False
    }
    
    # Ejecutar la inferencia y medir tiempo
    start_time = time.time()
    try:
        async with aiohttp.ClientSession() as session:
            async with session.post(f"{api_url}/api/generate", json=payload) as response:
                if response.status != 200:
                    error_text = await response.text()
                    print(f"‚ùå Error en la generaci√≥n: status {response.status}")
                    print(f"Detalles: {error_text}")
                    
                    # Intentar con endpoint alternativo (chat)
                    print("Intentando con endpoint alternativo (/api/chat)...")
                    chat_payload = {
                        "model": model,
                        "messages": [
                            {"role": "system", "content": "Eres un experto en hardware para inteligencia artificial."},
                            {"role": "user", "content": prompt}
                        ],
                        "options": options
                    }
                    
                    async with session.post(f"{api_url}/api/chat", json=chat_payload) as chat_response:
                        if chat_response.status != 200:
                            chat_error = await chat_response.text()
                            print(f"‚ùå Error con endpoint chat: {chat_response.status} - {chat_error}")
                            return None
                        
                        result = await chat_response.json()
                        response_text = result.get("message", {}).get("content", "")
                
                else:
                    result = await response.json()
                    response_text = result.get("response", "")
                
                end_time = time.time()
                elapsed = end_time - start_time
                
                # Mostrar resultados
                print(f"‚è±Ô∏è Tiempo de generaci√≥n: {elapsed:.2f} segundos")
                print(f"üìä Longitud de respuesta: {len(response_text)} caracteres")
                print(f"üìä Velocidad: {len(response_text) / elapsed:.2f} caracteres/segundo")
                
                # Mostrar fragmento de la respuesta
                preview_length = min(200, len(response_text))
                print(f"\nüìù Vista previa de respuesta:\n{response_text[:preview_length]}...\n")
                
                return {
                    "elapsed": elapsed,
                    "length": len(response_text),
                    "chars_per_second": len(response_text) / elapsed,
                    "text": response_text
                }
                
    except Exception as e:
        print(f"‚ùå Error durante la inferencia: {e}")
        return None

async def run_comparison(api_url, model):
    """Ejecuta una comparaci√≥n entre CPU y GPU para el mismo prompt y modelo"""
    print("\n" + "="*60)
    print("COMPARACI√ìN DE RENDIMIENTO CPU vs GPU")
    print("="*60)
    
    # Verificar que la API est√° respondiendo
    api_ok, models = await test_ollama_api(api_url)
    if not api_ok:
        print("‚ùå No se puede conectar con Ollama API. Abortando prueba.")
        return False
    
    # Verificar que el modelo existe
    model_names = [m.get('name') for m in models]
    if model not in model_names and models:
        print(f"‚ö†Ô∏è El modelo {model} no est√° disponible.")
        model = model_names[0]  # Usar el primer modelo disponible
        print(f"Usando modelo alternativo: {model}")
    
    # Prueba sin opciones GPU (CPU)
    cpu_result = await test_gpu_inference(api_url, model, use_gpu=False)
    if not cpu_result:
        print("‚ùå La prueba CPU fall√≥. No se puede continuar con la comparaci√≥n.")
        return False
    
    # Peque√±a pausa para asegurar recursos liberados
    await asyncio.sleep(2)
    
    # Prueba con opciones GPU
    gpu_result = await test_gpu_inference(api_url, model, use_gpu=True)
    if not gpu_result:
        print("‚ùå La prueba GPU fall√≥. Verificar configuraci√≥n de GPU para Ollama.")
        return False
    
    # Comparar resultados
    print("\n" + "="*60)
    print("RESULTADOS COMPARATIVOS")
    print("="*60)
    
    speedup = gpu_result["chars_per_second"] / cpu_result["chars_per_second"]
    time_ratio = cpu_result["elapsed"] / gpu_result["elapsed"]
    
    print(f"CPU: {cpu_result['elapsed']:.2f}s ({cpu_result['chars_per_second']:.2f} chars/s)")
    print(f"GPU: {gpu_result['elapsed']:.2f}s ({gpu_result['chars_per_second']:.2f} chars/s)")
    print(f"Aceleraci√≥n con GPU: {speedup:.2f}x")
    print(f"Reducci√≥n de tiempo: {(1 - gpu_result['elapsed'] / cpu_result['elapsed']) * 100:.1f}%")
    
    if speedup >= 1.5:
        print("\n‚úÖ La GPU est√° proporcionando una aceleraci√≥n significativa!")
        print(f"   {speedup:.1f}x m√°s r√°pido con GPU")
        return True
    elif speedup >= 1.1:
        print("\n‚úÖ La GPU proporciona algo de aceleraci√≥n, pero podr√≠a optimizarse m√°s.")
        print(f"   {speedup:.1f}x m√°s r√°pido con GPU")
        return True
    else:
        print("\n‚ö†Ô∏è La GPU no muestra aceleraci√≥n significativa. Verificar configuraci√≥n.")
        return False

async def test_ollama_gpu(api_url="http://ollama:11434", model="llama3:7b-instruct", run_comparison=False):
    """Prueba la detecci√≥n y uso de GPU por Ollama"""
    
    # Verificar conexi√≥n y modelos disponibles
    api_ok, models = await test_ollama_api(api_url)
    if not api_ok:
        return False
    
    # Verificar si el modelo solicitado existe
    model_names = [m.get('name') for m in models]
    if model not in model_names and models:
        print(f"‚ö†Ô∏è El modelo '{model}' no est√° disponible.")
        if models:
            model = model_names[0]  # Usar el primer modelo disponible
            print(f"Usando modelo alternativo: {model}")
        else:
            print("‚ùå No hay modelos disponibles para probar.")
            return False
    
    if run_comparison:
        # Ejecutar prueba comparativa
        return await run_comparison(api_url, model)
    else:
        # Ejecutar solo prueba con GPU
        result = await test_gpu_inference(api_url, model, use_gpu=True)
        return result is not None

async def main():
    parser = argparse.ArgumentParser(description="Prueba de GPU para Ollama")
    parser.add_argument("--url", default=os.environ.get("OLLAMA_API_BASE", "http://ollama:11434"), 
                        help="URL de la API de Ollama")
    parser.add_argument("--model", default="llama3", help="Modelo a probar")
    parser.add_argument("--host", action="store_true", help="Si se ejecuta desde el host, usar localhost")
    parser.add_argument("--compare", action="store_true", help="Realizar comparaci√≥n CPU vs GPU")
    args = parser.parse_args()
    
    # Si se ejecuta desde el host, utilizar localhost en lugar del nombre del contenedor
    if args.host and args.url == "http://ollama:11434":
        api_url = "http://localhost:11434"
        print(f"Ejecutando desde el host, usando {api_url}")
    else:
        api_url = args.url
    
    # Imprimir informaci√≥n inicial
    print("\n" + "="*60)
    print("PRUEBA DE FUNCIONALIDAD GPU PARA OLLAMA")
    print("="*60)
    print(f"URL API: {api_url}")
    print(f"Modelo: {args.model}")
    
    # Realizar prueba
    if args.compare:
        success = await run_comparison(api_url, args.model)
    else:
        success = await test_ollama_gpu(api_url=api_url, model=args.model)
    
    if success:
        print("\n‚úÖ PRUEBA EXITOSA: Ollama est√° funcionando correctamente con GPU")
        sys.exit(0)
    else:
        print("\n‚ùå PRUEBA FALLIDA: Hay problemas con la configuraci√≥n de Ollama o GPU")
        sys.exit(1)
        
if __name__ == "__main__":
    asyncio.run(main())